<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>所有文章 - Ethan's Blog</title><link>https://ethan-phu.github.io/posts/</link><description>所有文章 | Ethan's Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Fri, 06 Dec 2024 17:32:26 +0800</lastBuildDate><atom:link href="https://ethan-phu.github.io/posts/" rel="self" type="application/rss+xml"/><item><title>读付鹏和高善文对当前经济评论</title><link>https://ethan-phu.github.io/%E8%AF%BB%E4%BB%98%E9%B9%8F%E5%92%8C%E9%AB%98%E5%96%84%E6%96%87%E5%AF%B9%E5%BD%93%E5%89%8D%E7%BB%8F%E6%B5%8E%E8%AF%84%E8%AE%BA/</link><pubDate>Tue, 03 Dec 2024 18:07:13 +0800</pubDate><author>作者</author><guid>https://ethan-phu.github.io/%E8%AF%BB%E4%BB%98%E9%B9%8F%E5%92%8C%E9%AB%98%E5%96%84%E6%96%87%E5%AF%B9%E5%BD%93%E5%89%8D%E7%BB%8F%E6%B5%8E%E8%AF%84%E8%AE%BA/</guid><description><![CDATA[<p>今天读了付鹏先生在HSBC内部演讲的文稿，后面相继听了高善文先生的演讲。在阅读了高善文和付鹏关于中国经济形势的深刻分析后，我获得了对当前和未来经济趋势的更全面理解。</p>
<p>通过深入阅读高善文的分析，我深刻认识到经济转型和周期性压力是塑造中国经济未来的关键力量。经济转型引发的结构性变化是深远而持久的，它要求我们不断适应新的发展模式，比如从劳动密集型向技术密集型转变，从投资驱动向消费驱动转变。与此同时，周期性压力则在短期内对经济产生显著影响，如需求波动、市场信心变化等，这些都可能对我们的职业和财务状况产生直接或间接的影响。</p>
<p>这种理解使我意识到在不同的经济周期阶段，需要采取不同的应对策略。目前，我们的职业生涯将长期处于这个经济周期的尾声阶段，这意味着整体经济环境、就业环境以及收入增长潜力都不能与经济高速发展时期同日而语。为了适应这些变化，我们需要认真评估自己所在行业在经济转型中的位置，以及未来可能的发展趋势。如果行业前景黯淡，可能需要考虑转行或提升技能以适应新兴行业的需求。在经济增速放缓的背景下，我们也需要更加谨慎地管理个人财务，包括减少不必要的债务、增加储蓄和投资于相对稳定的资产。然后调整消费习惯，避免过度消费，尤其是在经济不确定性较高时期，理性消费变得更加重要。也是时候考虑要开启副业，增加职业以外的收入，以应对可能的经济波动。希望能够更好地应对经济转型和周期性压力带来的挑战，同时也为未来可能出现的新机遇做好准备。</p>
<p>作为普通人，我们需要建立更为全面和深入的经济理解，以便在不断变化的经济环境中做出明智的决策。这两篇文章为我未来的财务规划和职业发展提供了宝贵的指导。</p>
]]></description></item><item><title>2024年第46周, 患上桥本了</title><link>https://ethan-phu.github.io/2024%E5%B9%B4%E7%AC%AC46%E5%91%A8/</link><pubDate>Thu, 14 Nov 2024 21:20:46 +0800</pubDate><author>作者</author><guid>https://ethan-phu.github.io/2024%E5%B9%B4%E7%AC%AC46%E5%91%A8/</guid><description><![CDATA[<p>由于明天要去团建，后天一大早就要赶火车回长沙。所以周报今天先完成。后面每周都会写下我对生活的思考。</p>
<h2 id="本周的生活概述-">本周的生活概述 ：</h2>
<p>周六，参加了公司组织的年度体检。今年我对去年发现的甲状腺结节问题尤为关注，特意增加了甲功三项B专项检查。体检过程中，医生还建议我增加两项指标检测：抗甲状腺球蛋白抗体(TG-Ab)和甲状腺过氧化物酶抗体(TPO-Ab)，用于诊断是否患有桥本甲状腺炎。当天下午，血液检查结果就可以通过小程序同步查看结果显示我的TG-Ab高达78.14(IU/ml 正常范围0-4.11), TPO-Ab高达28.6(IU/ml 正常范围0-5.63)。</p>
<p>这对我来说，就是暴击，无疑就已经宣判了我患桥本了。后面仔细想了想，我体检前一天晚上没怎么睡好，且前一阵子不是吃烧烤就是出去喝奶茶，加上从媳妇老家带过来的辣椒酱爱不释手，可能这两个指标飙升和自身的生活习惯有关。在阅读了和桥本相关的医学知识后，感觉我从此要和辣椒无缘了，我可是正宗的湖南人啊，没有辣椒我能活？媳妇还在一旁不停的讲风凉话。不过我媳妇，也就是讲讲，心理比谁都更加重视我的健康。才30岁的我，身体就已经开始下滑，这让我开始反思自己。在这之前，我从来认为吃饭不就是一项任务？随意吃一点就好。以为自己很年轻，有更多的事情比吃饭，睡觉更加重要。现在想想，我真的有点大错特错了，对于现在的我们来说，其实最重要的是照顾好身体，身体才是我们的本钱，没有本钱，怎么去实现自己的价值呢？</p>
<h2 id="成长与学习-">成长与学习 ：</h2>
<ul>
<li><del>阅读完成《真需求》梁宁</del></li>
<li>阅读《亲密关系》罗兰.米勒 20%</li>
<li>阅读《桥本甲状腺炎90天治疗方案》20%</li>
</ul>
<h2 id="健康与自我关爱-">健康与自我关爱 ：</h2>
<h3 id="圆环闭合情况">圆环闭合情况：</h3>
<p>
自从检查出桥本后，我基本每天早上半小时运动，中午半小时运动，晚饭后半小时运动，然后调整饮食，一个月后再去复查，看看指标有没有下降或好转的可能。</p>
<h2 id="下周的计划-">下周的计划 ：</h2>
<p>下周准备回长沙，搞开荒，然后软装进场。终于房子装修告一段落了。诶，从买房到现在已经月光了一整年。希望月光的时间赶紧过去，然后尽自己最大的能力存钱。</p>
<h2 id="乐趣与感恩-">乐趣与感恩 ：</h2>
<p>从看到诊断结果到现在整整一周了，她每天坚决执行清淡饮食，督促我早睡早起，每天早上出门锻炼30分钟。在这里我非常感谢我媳妇在背后对我的支持。</p>
]]></description></item><item><title>当下的事情</title><link>https://ethan-phu.github.io/%E5%BD%93%E4%B8%8B%E4%B8%93%E6%B3%A8/</link><pubDate>Thu, 14 Nov 2024 08:10:11 +0800</pubDate><author>作者</author><guid>https://ethan-phu.github.io/%E5%BD%93%E4%B8%8B%E4%B8%93%E6%B3%A8/</guid><description><![CDATA[<h1 id="当下">当下</h1>
<p>本页记录当下我需要专注的事情。更新于2024/12/02 于中国武汉</p>
<h2 id="生活">生活</h2>
<ul>
<li>日常工作：练习专注，寻找目标感
<ul>
<li>项目稳步推进</li>
<li>测试同学的挑衅淡定对待，工作而已</li>
</ul>
</li>
<li>业余生活：稳定作息，健康生活
<ul>
<li>坚持做饭</li>
<li>有节奏的作息，拒绝熬夜</li>
</ul>
</li>
<li>运动健身：提高基础代谢
<ul>
<li>开始跑步，每周至少两次</li>
<li>继续羽毛球运动</li>
</ul>
</li>
</ul>
<h2 id="学习">学习</h2>
<ul>
<li>读书：
<ul>
<li>阅读《build a large language model from scratch》 60%</li>
<li><del>阅读《真需求》梁宁</del></li>
<li>阅读《亲密关系》罗兰.米勒 20%</li>
<li>阅读《桥本甲状腺炎90天治疗方案》20%</li>
<li>阅读《learning-ebpf》5%</li>
</ul>
</li>
<li>技术：
<ul>
<li>学习深度包解析技术</li>
<li>学习TCP协议相关知识</li>
<li>学习rust相关知识</li>
</ul>
</li>
<li>写作：
<ul>
<li>提升写作方面的能力</li>
</ul>
</li>
</ul>
<h2 id="项目">项目</h2>
<ul>
<li>流量采集器</li>
</ul>
]]></description></item><item><title>认证订阅</title><link>https://ethan-phu.github.io/%E8%AE%A4%E8%AF%81follow%E8%AE%A2%E9%98%85%E6%BA%90/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0800</pubDate><author>作者</author><guid>https://ethan-phu.github.io/%E8%AE%A4%E8%AF%81follow%E8%AE%A2%E9%98%85%E6%BA%90/</guid><description>&lt;p>This message is used to verify that this feed (feedId:71916462721158158) belongs to me (userId:45764741539537920). Join me in enjoying the next generation information browser &lt;a href="https://follow.is" target="_blank" rel="noopener noreffer">https://follow.is&lt;/a>.&lt;/p></description></item><item><title>写在28岁的中点</title><link>https://ethan-phu.github.io/%E5%86%99%E5%9C%A828%E5%B2%81%E7%9A%84%E4%B8%AD%E7%82%B9/</link><pubDate>Sun, 08 Jan 2023 00:00:00 +0800</pubDate><author>作者</author><guid>https://ethan-phu.github.io/%E5%86%99%E5%9C%A828%E5%B2%81%E7%9A%84%E4%B8%AD%E7%82%B9/</guid><description><![CDATA[<p>Dear Ethan:</p>
<p>又到了充满期待的新的一年，过去的一年你过得还好吗？在往年的年终总结中，你会对即将到来的一年许下满满的期待。很明显，去年的你又是欠下满满债务的你。选择在你28岁的中点写一封信给自己，这更私人，但也更贴近你的内心。</p>
<h2 id="命途多舛何以不甘">命途多舛，何以不甘</h2>
<p>又一年的时间，你经历了不同的事情，遇到了不同的人，了解了不同的故事，现在轮到你说一说自己的故事了。也许都听过关于西西弗斯的故事，他的一生就是不断将巨石推到山顶，又不得不经受巨石滚落，再将石头推向山顶，这样一个荒诞的周而复始的故事。这也许，也是我们每一个人所需要经历的人生。</p>
<p>三月到五月的你，在小论文、毕业论文修改和实验中度过，那时的你，年轻气盛，因为一点小小的观点和老师争吵的面红耳赤。六月份经历了毕业的狂欢，阔别了昔日一起学习的良师益友，与好友约定毕业旅行，因囊中羞涩与疫情的封控而取消。急忙奔赴职场，结实新的朋友，重新投入到自我的升华之中。总的来说，去年的你，经历了人生中的两件大事：毕业和工作，再次完成学生到职场人身份的转变，其它的都是一地鸡毛。</p>
<p>这一年中失去的东西太多太多，任何一点细小的死亡与崩坏都会变得不可承受，这大概就是去年的一个缩影吧，巨石一次次的滚动，我们一次次的再上路。真的很想努力，但满满的无力感。这种无力感，年复一年，细细沉思，最早可追溯到2015年，那是我第一次深刻体会这种无力感。如今七年已过，你仍旧在与这种无力感继续搏斗着。</p>
<p>此前的每一个人生阶段&mdash;-初中，高中，大学，似乎总是被安排着走的，大的方向永远是一年比一年好。那份不甘于现实的热情，还能继续保持，也许正是因为不曾经历大的挫折。仔细回忆过往的人生，之前的你确实保持着点自我。那会儿呢，只需要考虑自己就已经足够了，家人永远是不断给予付出的那一方，所以那会儿做什么事都是那么天真吧！这份自我得益于你的少年意气，得益于家庭给你的支撑，也得益于时代的滚滚向前。但人生或命运从来就没有承诺过谁，总会往更好的方向发展。巨石总会滚落，而明天一早睁眼，我们依旧需要推着巨石往上。</p>
<h2 id="肩负起自己的责任">肩负起自己的责任</h2>
<p>去年的你，每一天都在慌慌张张中度过，连家人都没能好好陪伴，也没有很好的意识到，父母的年纪已经到了颐养天年的时刻，我们需要无时不刻的关注着，陪伴着他们。而你每一天都在焦虑中挣扎，却无法鼓起勇气，让现在的你有所改观，因为你此刻内心是害怕的，害怕试错的代价太大，害怕失败，害怕被人嘲笑。可是，正如上面所说，人生或命运从来就没有承诺过谁，总会往更好的方向发展，所以今年的你，一定要鼓起勇气做出一点改变啦！</p>
<p>我知道在过去的一年，你无数次打开B站，似乎想要寻找什么答案，可是刷了很久，焦虑一点没减少。事实一次次的告诉你，既然别人无法明确的告诉你，那你就要学会戴着镣铐和生活共舞，不是吗？毛姆在写《月亮与六便士》的时候，大概忘了在理想和现实中间还有责任。他没有告诉你站在路口，抬头是月亮，低头是捡硬币，责任在肩膀上压着，那你该往哪儿走。你唯一确定的是，你想负起这个责任。因为曾经家人的支持是你的底气，你今天，同样想成为家人的底气。</p>
<h2 id="所谓成长接受自我">所谓成长，接受自我</h2>
<p>直到现在我才真正的意识到，所谓的成长就是认知的不断升级。只有当你明白这个道理，这个世界才开始真正的展开在你的眼前，原来以前认为错误的事情，原来也可以是对的「之前和老师争论的面红耳赤」。你不再为某一个你不认同的点去争论，慢慢的学会去理解别人，尊重别人，倾听大家的声音，不再自我，这已经是你最大的成长了。到了这个年纪才谈成长，这也许是一件过于奢侈的事情了。有很多很多的人，已经过早的品尝了世间的滋味。但对于刚入社会的我来说，考验才刚刚开始。成长不是随着年龄的增长，被社会打磨成一样的世故和圆滑，而是在生命的成熟中，仍有一颗纯真的童心和一颗善良的爱心。你想得到月亮，即使如此的平凡，不能起飞，也要努力的走着，跑着，伸手去够，去摘。即使经历过种种不顺，还是会有好事发生，会有新的缘分，新的身份，新的挑战，我不认输，你也不要，好吗？</p>
<h2 id="寄语未来">寄语未来</h2>
<p>2023年，愿你在不平和焦虑的时候，能记起你的初心和梦想，然后大踏步的坚持走向明天！！！</p>
]]></description></item><item><title>读《程序员修炼之道》</title><link>https://ethan-phu.github.io/%E8%AF%BB%E7%A8%8B%E5%BA%8F%E5%91%98%E4%BF%AE%E7%82%BC%E4%B9%8B%E9%81%93/</link><pubDate>Mon, 02 Jan 2023 00:00:00 +0800</pubDate><author>作者</author><guid>https://ethan-phu.github.io/%E8%AF%BB%E7%A8%8B%E5%BA%8F%E5%91%98%E4%BF%AE%E7%82%BC%E4%B9%8B%E9%81%93/</guid><description><![CDATA[<h2 id="务实的哲学">务实的哲学</h2>
<ul>
<li>
<p>团队信任对于创造力和协作至关重要，关键时刻信任的破坏几乎无法修复</p>
</li>
<li>
<p>提供选择，别找借口&ndash; 小黄鸭编程</p>
</li>
<li>
<p>破窗理论&ndash; 不要因为一些危急的事情，造成附加伤害，尽可能控制软件的熵</p>
</li>
<li>
<p>人们都觉得，加入一个推进中的成功项目更容易一些（煮石头汤的故事）</p>
</li>
<li>
<p>永远审视项目，不要做温水青蛙，先养成仔细观察周围环境的习惯，然后再项目中这样做</p>
</li>
<li>
<p>知识和经验是你最重要的资产，但是它们是时效资产，学习新事物的能力是你最重要的战略资产。 知识组合：</p>
<ol>
<li>
<p>定期投资&ndash;安排一个固定的时间和地点学习</p>
<ul>
<li>每年学习一门新语言</li>
<li>每月读一本技术书</li>
<li>读非技术书</li>
<li>上课&ndash; 了解公司之外的人都在做什么</li>
<li>尝试不同的环境</li>
<li>与时俱进&ndash;关心最新技术的进展</li>
</ul>
<p>想法的交叉是很重要的 批判性思维&ndash;批判性思考独到的和听到的东西</p>
</li>
<li>
<p>多样化&ndash; 熟悉的技能越多越好</p>
</li>
<li>
<p>风险管理&ndash;不同技术在高风险高回报到低风险低回报区间均匀分布，不要把技术鸡蛋放在一个篮子里</p>
</li>
<li>
<p>低买高卖&ndash;在一项新兴技术流行之前就开始学习，不过这是押宝</p>
</li>
<li>
<p>重新评估调整&ndash;不断刷新自己的知识库</p>
</li>
</ol>
</li>
<li>
<p>批判性思维</p>
<ol>
<li>五次为什么</li>
<li>谁从中收益</li>
<li>有什么背景</li>
<li>什么时候在哪里工作可以工作起来</li>
<li>为什么是这个问题</li>
</ol>
</li>
<li>
<p>写一个大纲， 问自己：这是否用正确的方式表达了我想表达的东西，以及现在是表达这个东西的好时机吗？</p>
</li>
</ul>
<h2 id="务实的方法">务实的方法</h2>
<h3 id="etceasy-to-change">ETC（easy to change）</h3>
<p>核心知道思想</p>
<ul>
<li>DRY(Don&rsquo;t repeat yourself)</li>
<li>正交性 良好设计中，数据库相关代码应该和用户界面保持正交， 当系统的组件相互之间高度依赖时，就没有局部修理这回事。</li>
</ul>
]]></description></item><item><title>Django源码系列：文件变化后server自动重启机制</title><link>https://ethan-phu.github.io/django%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B8%80/</link><pubDate>Tue, 28 Jun 2022 00:00:00 +0800</pubDate><author>作者</author><guid>https://ethan-phu.github.io/django%E6%BA%90%E7%A0%81%E7%B3%BB%E5%88%97%E4%B8%80/</guid><description><![CDATA[<h2 id="初试---文件变化后-server-自动重启">初试 - 文件变化后 <code>server</code> 自动重启</h2>
<p>本源码系列是基于 Django4.0 的源码，可以自行到<a href="https://github.com/django/django.git" target="_blank" rel="noopener noreffer">django 官方</a>下载。</p>
<blockquote>
<p>在此之前，不妨先了解下 <code>django</code> 是如何做到自动重启的</p>
</blockquote>
<h3 id="开始">开始</h3>
<p><code>django</code> 使用 <code>runserver</code> 命令的时候，会启动俩个进程。</p>
<p><code>runserver</code> 主要调用了 <code>django/utils/autoreload.py</code> 下 <code>main</code> 方法。<br>
<em>至于为何到这里的，我们这里不作详细的赘述，后面篇章会进行说明。</em></p>
<p>主线程通过 <code>os.stat</code> 方法获取文件最后的修改时间进行比较，继而重新启动 <code>django</code> 服务（也就是子进程）。</p>
<p>大概每秒监控一次。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># django/utils/autoreload.py 的 reloader_thread 方法</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">reloader_thread</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="o">...</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 监听文件变化</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># -- Start</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 这里主要使用了 `pyinotify` 模块，因为目前可能暂时导入不成功，使用 else 块代码</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># USE_INOTIFY 该值为 False</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">USE_INOTIFY</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">fn</span> <span class="o">=</span> <span class="n">inotify_code_changed</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">fn</span> <span class="o">=</span> <span class="n">code_changed</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># -- End</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="n">RUN_RELOADER</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">change</span> <span class="o">=</span> <span class="n">fn</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">change</span> <span class="o">==</span> <span class="n">FILE_MODIFIED</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># force reload</span>
</span></span><span class="line"><span class="cl">        <span class="k">elif</span> <span class="n">change</span> <span class="o">==</span> <span class="n">I18N_MODIFIED</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">reset_translations</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>code_changed</code> 根据每个文件的最后修改时间是否发生变更，则返回 <code>True</code> 达到重启的目的。</p>
<h3 id="父子进程多线程">父子进程&amp;多线程</h3>
<p>关于重启的代码在 <code>python_reloader</code> 函数内</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># django/utils/autoreload.py</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">restart_with_reloader</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 在这里开始设置环境变量为true</span>
</span></span><span class="line"><span class="cl">    <span class="n">new_environ</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">,</span> <span class="n">DJANGO_AUTORELOAD_ENV</span><span class="p">:</span> <span class="s2">&#34;true&#34;</span><span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="n">args</span> <span class="o">=</span> <span class="n">get_child_arguments</span><span class="p">()</span> <span class="c1">#获取执行的命令参数</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 重启命令在这里开始生效</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">p</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">new_environ</span><span class="p">,</span> <span class="n">close_fds</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">returncode</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">p</span><span class="o">.</span><span class="n">returncode</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">run_with_reloader</span><span class="p">(</span><span class="n">main_func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGTERM</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 刚开始 DJANGO_AUTORELOAD_ENV是没有被设置为true的所以这里会进入到else里。</span>
</span></span><span class="line"><span class="cl">    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">DJANGO_AUTORELOAD_ENV</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&#34;true&#34;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">reloader</span> <span class="o">=</span> <span class="n">get_reloader</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;Watching for file changes with </span><span class="si">%s</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">reloader</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
</span></span><span class="line"><span class="cl">            <span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">start_django</span><span class="p">(</span><span class="n">reloader</span><span class="p">,</span> <span class="n">main_func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="c1"># 开启django服务线程</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">exit_code</span> <span class="o">=</span> <span class="n">restart_with_reloader</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">sys</span><span class="o">.</span><span class="n">exit</span><span class="p">(</span><span class="n">exit_code</span><span class="p">)</span> <span class="c1"># 0为正常退出，其他的会抛出相关的错误</span>
</span></span><span class="line"><span class="cl">    <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">pass</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>程序启动，因为没有 <code>RUN_MAIN</code> 变量，所以走的 else 语句块。</p>
<p>颇为有趣的是，<code>restart_with_reloader</code> 函数中使用 <code>subprocess.run</code> 方法执行了启动程序的命令（ e.g：python3 manage.py runserver ），此刻 <code>RUN_MAIN</code> 的值为 <code>True</code> ，接着执行 <code>_thread.start_new_thread(main_func, args, kwargs)</code> 开启新线程，意味着启动了 <code>django</code> 服务。</p>
<p>如果子进程不退出，则停留在 <code>run</code> 方法这里（进行请求处理），如果子进程退出，退出码不是 3，while 则被终结。反之就继续循环，重新创建子进程。</p>
<h3 id="总结">总结</h3>
<p>以上就是 <code>django</code> 检测文件修改而达到重启服务的实现流程。</p>
<p>结合 <code>subprocess.run</code> 和 环境变量 创建俩个进程。主进程负责监控子进程和重启子进程。
子进程下通过开启一个新线程（也就是 <code>django</code> 服务）。主线程监控文件变化，如果变化则通过 <code>sys.exit(3)</code> 来退出子进程，父进程获取到退出码不是 3 则继续循环创建子进程，反之则退出整个程序。</p>
<p>好，到这里。我们勇敢的迈出了第一步，我们继续下一个环节！！！ ヾ(◍°∇°◍)ﾉﾞ</p>
]]></description></item><item><title>关于周报这种小事</title><link>https://ethan-phu.github.io/%E5%85%B3%E4%BA%8E%E5%BC%80%E5%90%AF%E5%91%A8%E6%8A%A5/</link><pubDate>Thu, 16 Jun 2022 00:00:00 +0800</pubDate><author>作者</author><guid>https://ethan-phu.github.io/%E5%85%B3%E4%BA%8E%E5%BC%80%E5%90%AF%E5%91%A8%E6%8A%A5/</guid><description><![CDATA[<p>从认识到双向链接开始我先使用了 obsidian,然而对于我这种懒癌晚期的人来说，需要结构化的记录，真的不太适合我「取一个好听的名字真的太难了」。当然有一说一，双链这个观点真的是太妙了。</p>
<p>现在的我已经全面转向了 logseq 来进行笔记记录，不得不说这种支持自定义代码的笔记真的不错「虽然我到目前为止使用最多的是<strong>高级查询</strong>和<strong>TODO</strong>这两个功能，记录笔记的话只是零星记录了几句话，并没有详细的记录或者输出一些东西。」。在 logseq 中是以日期为主线的，这免去了我对要写的内容的抽象主题的负担。</p>
<h2 id="工作日报周报">工作日报周报</h2>
<p>首先对任务进行分类，在学习了 Logseq 的高级查询语法并了解其能力之后，我决定使用<code>#tag</code>这种形式来组织个人任务和工作任务，属于工作任务的会标记上<code>#work</code>标签，个人任务就不进行标签。</p>
<p>首先就是工作，这个是大块内容，我目前使用 logseq 生成<strong>日报&amp;周报</strong>，用于之后提交到绩效评估系统中。</p>
<h3 id="日报周报代码">日报/周报代码</h3>
<p>日报和周报代码上只有 inputs[:yesterday]更改成 inputs[:7d]即可。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">#+BEGIN_QUERY
</span></span><span class="line"><span class="cl">{
</span></span><span class="line"><span class="cl">    :title &#34;查询昨天完成的任务&#34;
</span></span><span class="line"><span class="cl">    :query [:find (pull ?b [*])
</span></span><span class="line"><span class="cl">       :in $ ?start ?end
</span></span><span class="line"><span class="cl">       :where
</span></span><span class="line"><span class="cl">       [?b :block/marker ?m]
</span></span><span class="line"><span class="cl">       [?b :block/page ?p]
</span></span><span class="line"><span class="cl">       [?p :page/journal? true]
</span></span><span class="line"><span class="cl">       [?p :page/journal-day ?d]
</span></span><span class="line"><span class="cl">       [(&gt;= ?d ?start)]
</span></span><span class="line"><span class="cl">       [(&lt;= ?d ?end)]
</span></span><span class="line"><span class="cl">       [?b :block/path-refs [:block/name &#34;work&#34;]]
</span></span><span class="line"><span class="cl">       [(= &#34;DONE&#34; ?m)]
</span></span><span class="line"><span class="cl">   ]
</span></span><span class="line"><span class="cl">   :inputs [:yesterday :today]
</span></span><span class="line"><span class="cl">}
</span></span><span class="line"><span class="cl">#+END_QUERY
</span></span></code></pre></td></tr></table>
</div>
</div><p>效果图如下：
</p>
]]></description></item><item><title>Randy Pausch 教授的最后一课</title><link>https://ethan-phu.github.io/%E5%85%B0%E8%BF%AA%E6%95%99%E6%8E%88%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E8%AF%BE/</link><pubDate>Sun, 29 May 2022 00:00:00 +0800</pubDate><author>作者</author><guid>https://ethan-phu.github.io/%E5%85%B0%E8%BF%AA%E6%95%99%E6%8E%88%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E8%AF%BE/</guid><description><![CDATA[<p>在重听李沐老师的《<a href="https://www.youtube.com/watch?v=NnSldWhSqvY&amp;list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I&amp;index=4" target="_blank" rel="noopener noreffer">Resnet 论文精读</a>》这一课的时候，ps:「之前没有好好读，:&gt;)羞耻」。在讲到双栏论文中第一页的第二栏最上面，这个位置在学术界是非常重要的。提到了 Randy Pausch 教授在图形学开创了这一风格的写法，然后提到了他的最后一课「深刻印象」。</p>
<p>于是我从 cmu 网站中找到了当年演讲的<a href="https://www.cmu.edu/randyslecture/" target="_blank" rel="noopener noreffer">材料</a>，并完整的看了视频，这份笔记是 Randy 的人生经验和建议的抄录。</p>
<h2 id="人生经验">人生经验</h2>
<blockquote>
<ol>
<li>Have something to bring to the table, because that will make you more welcom.
你必须要有一些真本领，这样可以让你更受欢迎。</li>
</ol>
</blockquote>
<blockquote>
<ol start="2">
<li>You&rsquo;ve got to get the fundamentals down because otherwise the fancy stuff isn&rsquo;t going to work.
你必须练好基本功，否则后面的事情都不会发生。</li>
</ol>
</blockquote>
<blockquote>
<ol start="3">
<li>That was a bit of a setback. But remember, the brick walls are there for a reason. The brick walls are not there to keep us out. The brick walls are there to give us a chance to show how badly we want something. Becuase the brick walls are there to stop the people who don&rsquo;t want it badly enough. They&rsquo;re there to stop the other people. Remember brick walls let us show our dedication. They are there to separate us from the people who don&rsquo;t really want to achieve their childhood dreams.
你总会遇到挫折。但是记住，它们的出现不是没有原因的。砖墙并不是为了挡住我们。它在那里，只是为了测试，我们的决心到底有多迫切。它在那里挡住那些没有强烈决心的人。它不让那些人通过。记住，砖墙的存在是为了显示我们自己付出的决心。它使得我们，同那些并不真的想实现梦想的人得以区分。</li>
</ol>
</blockquote>
<h2 id="为人处世的建议">为人处世的建议</h2>
<blockquote>
<ol>
<li>helping others. 帮助他人。</li>
</ol>
</blockquote>
<blockquote>
<ol start="2">
<li>never lose the childlike wonder. It&rsquo;s what drives us. 永远不要失去好奇心，它是人类前进的动力。</li>
</ol>
</blockquote>
<blockquote>
<ol start="3">
<li>Loyalty is a two way street. 诚以待人，这样别人也会忠实的对待你。</li>
</ol>
</blockquote>
<blockquote>
<ol start="4">
<li>Never give up. 永远不要放弃</li>
</ol>
</blockquote>
<blockquote>
<ol start="5">
<li>You can&rsquo;t get there alone. People have to help you. You get people to help you by telling the truth. 你不能单打独斗，必须有人来帮你。只要你讲真话，就会有人来帮你。</li>
</ol>
</blockquote>
<blockquote>
<ol start="6">
<li>Apologize when you screw up and focus on other people, not on yourself. 当你把事情搞砸，首先要向别人道歉，首先关心他们的损失，而不是你自己的损失。</li>
</ol>
</blockquote>
<blockquote>
<ol start="7">
<li>When you do the right thing, good stuff has a way of happening. 如果你做了正确的事，好的结果自然会发生。</li>
</ol>
</blockquote>
<blockquote>
<ol start="8">
<li>Get a feedback loop and listen to it. 注意倾听反馈。</li>
</ol>
</blockquote>
<blockquote>
<ol start="9">
<li>Show gratitude. 感恩</li>
</ol>
</blockquote>
<blockquote>
<ol start="10">
<li>Don&rsquo;t complain. Just work harder. 不要抱怨，而要加倍努力。</li>
</ol>
</blockquote>
<blockquote>
<ol start="11">
<li>Be good at something, it makes you valuable. 要有一技之长，它使你有价值。</li>
</ol>
</blockquote>
<blockquote>
<ol start="12">
<li>Work hard. 努力再努力。</li>
</ol>
</blockquote>
<blockquote>
<ol start="13">
<li>Find the best in everybody. 注意发现他人的优点。</li>
</ol>
</blockquote>
<blockquote>
<ol start="14">
<li>Be prepared. Luck is truly where preparation meets opportunity. 做好准备。所谓幸运，真的是机会和准备的结合。</li>
</ol>
</blockquote>
]]></description></item><item><title>transformer源码阅读</title><link>https://ethan-phu.github.io/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</link><pubDate>Sat, 28 May 2022 00:00:00 +0800</pubDate><author>作者</author><guid>https://ethan-phu.github.io/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</guid><description><![CDATA[<p>本文介绍 Tranformer 的代码。</p>
<h2 id="模型结构">模型结构</h2>
<p>Encoder 将输入序列$(x_{1},\cdots,x_{n})$ 映射成一个连续的序列$z = (z_{1},\cdots,z_{n})$。而 Decoder 根据$z$来解码得到输出序列$(y_{1},\cdots,y_{m})$。Decoder 是自回归的(auto-regressive)&ndash;它会把前一个时刻的输出作为当前时刻的输入。Encoder-Decoder 结构模型的代码如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">	标准的Encoder-Decoder架构。
</span></span></span><span class="line"><span class="cl"><span class="s2">	&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">src_embed</span><span class="p">,</span> <span class="n">tgt_embed</span><span class="p">,</span> <span class="n">generator</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="nb">super</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">,</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># 源语言和目标语言的embedding</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span> <span class="o">=</span> <span class="n">src_embed</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span> <span class="o">=</span> <span class="n">tgt_embed</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># generator主要是根据Decoder的隐状态输出当前时刻的词(单个词)</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># 基本的实现就是隐状态输入一个全连接层，然后接一个softmax变成概率</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">generator</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># 首先调用encode方法对输入进行编码，然后调用decode方法进行解码</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># 调用self.encoder函数</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">src_embed</span><span class="p">(</span><span class="n">src</span><span class="p">),</span> <span class="n">src_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># 调用self.decoder函数 注意⚠️：这里定义的memery是encoder的输出结果。</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tgt_embed</span><span class="p">(</span><span class="n">tgt</span><span class="p">),</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>EncoderDecoder 定义了一种通用的 Encoder-Decoder 架构，具体的 Encoder、Decoder、src_embed、target_embed 和 generator 都是构造函数传入的参数。这样我们做实验更换不同的组件就会更加方便。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Generator</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="nb">super</span><span class="p">(</span><span class="n">Generator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># d_model是Decoder输出的大小，vocab是词典的大小</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>注意 ⚠️：Generator 返回的是 softmax 的 log 值。在 pytorch 中为了计算交叉熵损失，有两种方法。第一种方法就是 nn.CrossEntropyLoss(),一种是使用 NLLLoss()。第一种方法更加容易懂，但是在很多开源代码里第二种更常见。</p>
<blockquote>
<p>CrossEntropyLoss:</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># 服从0-1的正太分布。</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>比如上面的代码，假设是 5 分类问题，<code>x</code>表示模型的输出 logits(batch=1)，而 y 是真实分类的下标(0-4)。实际的计算过程为：$loss = -\sum^{5}<em>{i=1}y</em>{i}log(softmax(x_{i}))$。</p>
<blockquote>
<p>NLLLoss(Negative Log Likelihood Loss)是计算负 log 似然损失。它输入的 x 是 log_softmax 之后的结果（长度为 5 的数组），y 是真实分类（0-4），输出就是 x[y]。因此代码为：</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">m</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">random_</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Transformer 模型也是遵循上面的架构，只不过它的 Encoder 是 N(6)个 EncoderLayer 组成，每个 EncoderLayer 包含一个 Self-Attention SubLayer 层和一个全连接 SubLayer 层。而它的 Decoder 也是 N(6)个 DecoderLayer 组成，每个 DecoderLayer 包含一个 Self-Attention SubLayer 层、Attention SubLayer 层和全连接 SubLayer 层。如下图所示。</p>
<p></p>
<h2 id="encoder-和-decoder-stack">Encoder 和 Decoder Stack</h2>
<p>前面说了 Encoder 和 Decoder 都是由 N 个相同结构的 Layer 堆积(stack)而成。因此我们首先定义 clones 函数，用于克隆相同的 SubLayer。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">clones</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="c1"># 克隆N个完全相同的SubLayer，使用了copy.deepcopy</span>
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这里使用了 nn.ModuleList, ModuleList 就像一个普通的 Python 的 List，我们可以使用下标来访问它，它的好处是传入的 ModuleList 的所有 Module 都会注册的 PyTorch 里，这样 Optimizer 就能找到这里面的参数，从而能够用梯度下降更新这些参数。但是 nn.ModuleList 并不是 Module（的子类），因此它没有 forward 等方法，我们通常把它放到某个 Module 里。接下来定义 Encoder:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="c1"># Encoder是N个EncoderLayer的stack</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># layer是一个SubLayer，我们clone N个</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># 再加一个LayerNorm层</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># 逐层进行处理</span>
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">			<span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># 最后进行LayerNorm</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Encoder 就是 N 个 SubLayer 的 stack，最后加上一个 LayerNorm。我们来看 LayerNorm:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="nb">super</span><span class="p">(</span><span class="n">LayerNorm</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">a_2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">features</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">b_2</span><span class="o">.</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">feagures</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">std</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">a_2</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">std</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b_2</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>LayerNorm：假设数据为[batch_size, unit, 1, features]，这里是对整个样本进行 normalization。这里的 Layer Normalization 不是 Batch Normalization。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">x -&gt; attention(x) -&gt; x+self-attention(x)[残差] -&gt; layernorm(x+self-attention(x)) =&gt; y
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">y -&gt; dense(y) -&gt; y+dense(y) -&gt; layernorm(y+dense(y)) =&gt; z(输入下一层)
</span></span></code></pre></td></tr></table>
</div>
</div><p>这里稍微做了一点修改， 在 self-attention 和 dense 之后加了一个 dropout 层。另一个不同支持就是把 layernorm 层放到前面了。这里的模型为：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">x -&gt; layernorm(x) -&gt; attention(layernorm(x)) -&gt; a + attention(layernorm(x)) =&gt; y
</span></span><span class="line"><span class="cl">y -&gt; layernorm(y) -&gt; dense(layernorm(y)) -&gt; y+dense(layernorm(y))
</span></span></code></pre></td></tr></table>
</div>
</div><p>原始论文的 layernorm 放在最后；而这里把它放在最前面并且在 Encoder 的最后在加了一个 layernorm。这里的实现和论文的实现基本是一致的，只是给最底层的输入 x 多做了一个 LayerNorm，而原始论文是没有的。下面是 Encoder 中的 forward 方法，这样比读者可能会比较清楚为什么 N 个 EncoderLayer 处理完成后还需要一个 LayerNorm。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">		<span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>不管是 Self-Attention 还是全连接层，都首先是 LayerNorm，然后是 Self-Attention/Dense，然后是 Dropout，最好是残差连接。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">SublayerConnection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">	LayerNorm+sublayer(Self-Attention/Dense) + dropout + 残差连接
</span></span></span><span class="line"><span class="cl"><span class="s2">	为了简单，把LayerNorm放到了前面，这和原始论文稍有不同，原始论文LayerNorm在最后。
</span></span></span><span class="line"><span class="cl"><span class="s2">	&#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="n">supper</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Droupout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">sublayer</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># sublayer是传入的参数,之后进行残差连接</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="n">x</span><span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">sublayer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Self-Attention 或者 Dense 并不在这里进行构造，而是放在了 EncoderLayer 里，在 forward 的时候由 EncoderLayer 传入。这样的好处是更加通用，比如 Decoder 也是类似的需要在 Self-Attention、Attention 或者 Dense 前面加上 LayerNorm 和 Dropout 以及残差连接，我们就可以复用代码。但是这里要求传入的 sublayer 可以使用一个参数来调用的函数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="c1"># EncoderLayer由self-attn和feed_forward组成</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">mask</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>为了复用，这里的 self_attn 层和 feed_forward 层也是传入的参数，这里只构造两个 SublayerConnection。forward 调用 sublayer<a href="%e8%bf%99%e6%98%afSubLayerConnection%e5%af%b9%e8%b1%a1" rel="">0</a>的<strong>call</strong>方法，最终会调到它的 forward 方法，而这个方法需要两个参数，一个是输入 Tensor， 一个是一个 callable, 并且这个 callable 可以用一个参数来调用。而 self_attn 函数需要 4 个参数（Query 的输入，key 的输入，Value 的输入和 Mask），因此这里我们使用 lambda 的技巧把它变成一个参数 x 的函数(mask 可以堪称已知的数)。因为 lambda 的形参也叫 x.</p>
<blockquote>
<p>Decoder</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="nb">super</span><span class="p">(</span><span class="n">Decoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">			<span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>Decoder 也是 N 个 DecoderLayer 的 stack，参数 layer 是 DecoderLayer，它也是一个 callable，最终<strong>call</strong>会调用 DecoderLayer.forward 方法，这个方法需要 4 个参数，输入 x, Encoder 层的输出 memory, 输入 Encoder 的 Mask(src_mask)和输入 Decoder 的 Mask(tgt_mask)。所有这里的 Decoder 的 forward 也需要 4 个参数。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="c1"># Decoder包括self-attn, src-attn, feed_forward</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">self_attn</span><span class="p">,</span> <span class="n">src_attn</span><span class="p">,</span> <span class="n">feed_forward</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="nb">super</span><span class="p">(</span><span class="n">DecoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">size</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">self_attn</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span> <span class="o">=</span> <span class="n">src_attn</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">feed_forward</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SublayerConnection</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">dropout</span><span class="p">),</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="n">m</span> <span class="o">=</span> <span class="n">memory</span>
</span></span><span class="line"><span class="cl">		<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">tgt_mask</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">		<span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">,</span><span class="n">src_mask</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sublayer</span><span class="p">[</span><span class="mi">2</span><span class="p">](</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>DecoderLayer 比 EncoderLayer 多了一个 src-attn 层，这是 Decoder 时 attend to Encoder 的输出(memory)。src_attn 和 self_attn 的实现是一样的，只不过使用的 Query, Key 和 Value 的输入不同。普通的 Attention(src_attn)的 Query 是从下层输入进行来的。 Key 和 Value 是 Encoder 最后一层的输出 memory;而 Self-Attention 的 Query, Key 和 Value 都是来自下层输入进来的。</p>
<p>Decoder 和 Encoder 有一个关键的不同：Decoder 在解码第 t 个时刻的时候只能用$1 \cdots t$时刻的输入，而不能使用$t+1$时刻及其之后的输入。因此我们需要一个函数来产生一个 Mask 矩阵,代码如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">subsequent_mask</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="c1"># mask out subsequent positoins</span>
</span></span><span class="line"><span class="cl">	<span class="n">attn_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="n">subsequent_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">attn_shape</span><span class="p">),</span><span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">subsequent_mask</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们阅读代码之前先看它的输出：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">subsequent_mask</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出</span>
</span></span><span class="line"><span class="cl"><span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span> <span class="mi">1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们发现它输出的是一个方阵，对角线和下面都是 1。第一行只有第一列是 1，它的意思是时刻 1 只能 attend to 输入 1， 第三行说明时刻 3 可以 attend to ${1, 2, 3 }$而不能 attend to ${4,5}$的输入，因为在真正 Decoder 的时候这是属于 Future 的信息。</p>
<h2 id="multiheadedattention-多头注意力机制">MultiHeadedAttention 多头注意力机制</h2>
<p>Attention(包括 Self-Attention 和普通的 Attention)可以堪称一个函数，它的输入是 Query，Key，Value 和 Mask，输出是一个 Tensor。其中输出是 Value 的加权平均，而权重来自 Query 和 Key 的计算。具体的计算如下图所示，计算公式为：$$Attention(Q,K,V) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$$</p>
<p></p>
<p>代码为：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span><span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">		<span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">mask_fill</span><span class="p">(</span><span class="n">mask</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="n">p_attn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">		<span class="n">p_attn</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">p_attn</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span><span class="n">value</span><span class="p">),</span><span class="n">p_attn</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>这里主要的疑问是在<code>score.mask_fill</code>，主要用于把 mask 是 0 的变成一个很小的数，这样后面经过 softmax 之后的概率就很接近零（但是理论上还是用了很少一点点未来的信息）。</p>
<p>之前介绍过，对于每一个 Head，都是用三个矩阵$W^{Q}$，$W^{K}$，$W^{V}$把输入转换成 Q，K 和 V。然后分别用每一个 Head 进行 Self- Attention 的计算，最后把 N 个 Head 的输出拼接起来，最后用一个矩阵$W^{O}$把输出压缩一下。具体计算框架为：</p>
<p></p>
<p>代码如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">		<span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">h</span> <span class="o">==</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">h</span> <span class="c1"># 这里是整除</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span><span class="n">d_k</span><span class="p">),</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="kc">None</span>
</span></span><span class="line"><span class="cl">		<span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">	<span class="k">def</span> <span class="nf">foward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">		<span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">			<span class="c1"># 所有h个head的mask都是相同的</span>
</span></span><span class="line"><span class="cl">			<span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="n">nbatches</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># 1) 首先使用线性变换，然后把d_model分配给h个Head，每个head为d_k=d_model/h</span>
</span></span><span class="line"><span class="cl">		<span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span><span class="p">,</span><span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">,(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))]</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># 2) 使用attention函数计算</span>
</span></span><span class="line"><span class="cl">		<span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span><span class="n">dropout</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="c1"># 3）</span>
</span></span><span class="line"><span class="cl">		<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">		<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>我们首先来看构造函数， 这里 d_model(512)是 Multi-Head 的输出大小，因为有 h(8)个 head， 因此每个 head 的 d_k=512/8=64。接着我们构造 4 个(d_model $*$ d_model)的矩阵，后面我们会看到它的用处。最后是构造一个 Dropout 层。</p>
<p>然后我们来看 forward 方法。输入的 mask 是（batch,1,time）的，因为每个 head 的 mask 都是一样的，所以先用 unsqueeze(1)变成(batch,1,1,time)，mask 我们前面已经分析过了。</p>
<p>接下来就是根据输入的 query, key, value 计算变换后的 Multi-Head 的 query, key 和 value。<code>zip(self.linears, (query,key,value))</code>是把<code>(self.linear[0], self.linear[1], self.linear[2])</code>和<code>(query, key, value)</code>放在一起遍历。我们可以只看一个<code>self.linear[0] (query)</code>。根据构造函数的定义，<code>self.linear[0]</code>是一个<code>[512,512]</code>的矩阵，而<code>query</code>是<code>(batch, time, 512)</code>，相乘之后得到的新 query 还是 512 维的向量，然后用 view 把它变成<code>(batch, time, 8, 64)</code>。然后 transpose 成(batch, 8, time, 64)，这是 attention 函数要求的 shape。分别对 8 个 Head，每个 Head 的 Query 都是 64 维。最后使用<code>self.linear[-1]</code>对<code>x</code>进行线性变换，<code>self.linear[-1]</code>是<code>[512, 512]</code>的，因此最终的输出还是<code>(batch, time, 512)</code>。</p>
]]></description></item></channel></rss>