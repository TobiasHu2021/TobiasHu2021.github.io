<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>transformer源码阅读 - Ethan's Blog</title><meta name=Description content="This is my cool site"><meta property="og:title" content="transformer源码阅读"><meta property="og:description" content="本文介绍 Tranformer 的代码。 模型结构 Encoder 将输入序列$(x_{1},\cdots,x_{n})$ 映射成一个连续的序列$z = (z_{1},\cdots,z_"><meta property="og:type" content="article"><meta property="og:url" content="https://ethan-phu.github.io/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-28T00:00:00+08:00"><meta property="og:site_name" content="My cool site"><meta name=twitter:card content="summary"><meta name=twitter:title content="transformer源码阅读"><meta name=twitter:description content="本文介绍 Tranformer 的代码。 模型结构 Encoder 将输入序列$(x_{1},\cdots,x_{n})$ 映射成一个连续的序列$z = (z_{1},\cdots,z_"><meta name=application-name content="My cool site"><meta name=apple-mobile-web-app-title content="My cool site"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://ethan-phu.github.io/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/><link rel=prev href=https://ethan-phu.github.io/%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%8F%91%E5%B1%95%E7%9A%84%E6%80%9D%E8%80%83/><link rel=next href=https://ethan-phu.github.io/%E5%85%B0%E8%BF%AA%E6%95%99%E6%8E%88%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E8%AF%BE/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"transformer源码阅读","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/ethan-phu.github.io\/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB\/"},"genre":"posts","keywords":"技术, 深度学习, 笔记, 源码阅读","wordcount":4380,"url":"https:\/\/ethan-phu.github.io\/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB\/","datePublished":"2022-05-28T00:00:00+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"ethan"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Ethan's Blog">Ethan</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/ title=文章>文章 </a><a class=menu-item href=/weekly title=兴趣周刊>周刊 </a><a class=menu-item href=/secnet title=网络安全研究>网络安全研究 </a><a class=menu-item href=/code title=编程杂谈>编程杂谈 </a><a class=menu-item href=/about/ title=关于ethan>关于 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Ethan's Blog">Ethan</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/ title=文章>文章</a><a class=menu-item href=/weekly title=兴趣周刊>周刊</a><a class=menu-item href=/secnet title=网络安全研究>网络安全研究</a><a class=menu-item href=/code title=编程杂谈>编程杂谈</a><a class=menu-item href=/about/ title=关于ethan>关于</a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">transformer源码阅读</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=https://github.com/ethan-phu title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>ethan</a></span>&nbsp;<span class=post-category>收录于 <a href=/categories/%E6%8A%80%E6%9C%AF/><i class="far fa-folder fa-fw" aria-hidden=true></i>技术</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2022-05-28>2022-05-28</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;约 4380 字&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;预计阅读 9 分钟&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#模型结构>模型结构</a></li><li><a href=#encoder-和-decoder-stack>Encoder 和 Decoder Stack</a></li><li><a href=#multiheadedattention-多头注意力机制>MultiHeadedAttention 多头注意力机制</a></li></ul></nav></div></div><div class=content id=content><p>本文介绍 Tranformer 的代码。</p><h2 id=模型结构>模型结构</h2><p>Encoder 将输入序列$(x_{1},\cdots,x_{n})$ 映射成一个连续的序列$z = (z_{1},\cdots,z_{n})$。而 Decoder 根据$z$来解码得到输出序列$(y_{1},\cdots,y_{m})$。Decoder 是自回归的(auto-regressive)&ndash;它会把前一个时刻的输出作为当前时刻的输入。Encoder-Decoder 结构模型的代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>EncoderDecoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>	标准的Encoder-Decoder架构。
</span></span></span><span class=line><span class=cl><span class=s2>	&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>encoder</span><span class=p>,</span> <span class=n>decoder</span><span class=p>,</span> <span class=n>src_embed</span><span class=p>,</span> <span class=n>tgt_embed</span><span class=p>,</span> <span class=n>generator</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>(</span><span class=n>EncoderDecoder</span><span class=p>,</span><span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>encoder</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>decoder</span> <span class=o>=</span> <span class=n>decoder</span>
</span></span><span class=line><span class=cl>		<span class=c1># 源语言和目标语言的embedding</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>src_embed</span> <span class=o>=</span> <span class=n>src_embed</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>tgt_embed</span> <span class=o>=</span> <span class=n>tgt_embed</span>
</span></span><span class=line><span class=cl>		<span class=c1># generator主要是根据Decoder的隐状态输出当前时刻的词(单个词)</span>
</span></span><span class=line><span class=cl>		<span class=c1># 基本的实现就是隐状态输入一个全连接层，然后接一个softmax变成概率</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>generator</span> <span class=o>=</span> <span class=n>generator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>tgt</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=c1># 首先调用encode方法对输入进行编码，然后调用decode方法进行解码</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>src</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>),</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>encode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>src</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=c1># 调用self.encoder函数</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>src_embed</span><span class=p>(</span><span class=n>src</span><span class=p>),</span> <span class=n>src_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>decode</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=c1># 调用self.decoder函数 注意⚠️：这里定义的memery是encoder的输出结果。</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>decoder</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>tgt_embed</span><span class=p>(</span><span class=n>tgt</span><span class=p>),</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>EncoderDecoder 定义了一种通用的 Encoder-Decoder 架构，具体的 Encoder、Decoder、src_embed、target_embed 和 generator 都是构造函数传入的参数。这样我们做实验更换不同的组件就会更加方便。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Generator</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>vocab</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>(</span><span class=n>Generator</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=c1># d_model是Decoder输出的大小，vocab是词典的大小</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>vocab</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>F</span><span class=o>.</span><span class=n>log_softmax</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>proj</span><span class=p>(</span><span class=n>x</span><span class=p>),</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>注意 ⚠️：Generator 返回的是 softmax 的 log 值。在 pytorch 中为了计算交叉熵损失，有两种方法。第一种方法就是 nn.CrossEntropyLoss(),一种是使用 NLLLoss()。第一种方法更加容易懂，但是在很多开源代码里第二种更常见。</p><blockquote><p>CrossEntropyLoss:</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>5</span><span class=p>)</span> <span class=c1># 服从0-1的正太分布。</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>empty</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>)</span><span class=o>.</span><span class=n>random_</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>y</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>比如上面的代码，假设是 5 分类问题，<code>x</code>表示模型的输出 logits(batch=1)，而 y 是真实分类的下标(0-4)。实际的计算过程为：$loss = -\sum^{5}<em>{i=1}y</em>{i}log(softmax(x_{i}))$。</p><blockquote><p>NLLLoss(Negative Log Likelihood Loss)是计算负 log 似然损失。它输入的 x 是 log_softmax 之后的结果（长度为 5 的数组），y 是真实分类（0-4），输出就是 x[y]。因此代码为：</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>m</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>log_softmax</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>criterion</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>NLLLoss</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>empty</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>dtype</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>)</span><span class=o>.</span><span class=n>random_</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>criterion</span><span class=p>(</span><span class=n>m</span><span class=p>(</span><span class=n>x</span><span class=p>),</span> <span class=n>y</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Transformer 模型也是遵循上面的架构，只不过它的 Encoder 是 N(6)个 EncoderLayer 组成，每个 EncoderLayer 包含一个 Self-Attention SubLayer 层和一个全连接 SubLayer 层。而它的 Decoder 也是 N(6)个 DecoderLayer 组成，每个 DecoderLayer 包含一个 Self-Attention SubLayer 层、Attention SubLayer 层和全连接 SubLayer 层。如下图所示。</p><p><img class=lazyload src=/svg/loading.min.svg data-src=http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_14_0.png data-srcset="http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_14_0.png, http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_14_0.png 1.5x, http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_14_0.png 2x" data-sizes=auto alt=http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_14_0.png title=transformer的结构图></p><h2 id=encoder-和-decoder-stack>Encoder 和 Decoder Stack</h2><p>前面说了 Encoder 和 Decoder 都是由 N 个相同结构的 Layer 堆积(stack)而成。因此我们首先定义 clones 函数，用于克隆相同的 SubLayer。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>clones</span><span class=p>(</span><span class=n>module</span><span class=p>,</span> <span class=n>N</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=c1># 克隆N个完全相同的SubLayer，使用了copy.deepcopy</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>copy</span><span class=o>.</span><span class=n>deepcopy</span><span class=p>(</span><span class=n>module</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>N</span><span class=p>)])</span>
</span></span></code></pre></td></tr></table></div></div><p>这里使用了 nn.ModuleList, ModuleList 就像一个普通的 Python 的 List，我们可以使用下标来访问它，它的好处是传入的 ModuleList 的所有 Module 都会注册的 PyTorch 里，这样 Optimizer 就能找到这里面的参数，从而能够用梯度下降更新这些参数。但是 nn.ModuleList 并不是 Module（的子类），因此它没有 forward 等方法，我们通常把它放到某个 Module 里。接下来定义 Encoder:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Encoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=c1># Encoder是N个EncoderLayer的stack</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>layer</span><span class=p>,</span> <span class=n>N</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>(</span><span class=n>Encoder</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=c1># layer是一个SubLayer，我们clone N个</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>clones</span><span class=p>(</span><span class=n>layer</span><span class=p>,</span> <span class=n>N</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=c1># 再加一个LayerNorm层</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>layer</span><span class=o>.</span><span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=c1># 逐层进行处理</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=c1># 最后进行LayerNorm</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Encoder 就是 N 个 SubLayer 的 stack，最后加上一个 LayerNorm。我们来看 LayerNorm:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>LayerNorm</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>features</span><span class=p>,</span> <span class=n>eps</span><span class=o>=</span><span class=mf>1e-6</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>(</span><span class=n>LayerNorm</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>a_2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>features</span><span class=p>))</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>b_2</span><span class=o>.</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>feagures</span><span class=p>))</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>eps</span> <span class=o>=</span> <span class=n>eps</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>mean</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>std</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>std</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span> <span class=o>=</span> <span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>a_2</span> <span class=o>*</span> <span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>mean</span><span class=p>)</span><span class=o>/</span><span class=p>(</span><span class=n>std</span><span class=o>+</span><span class=bp>self</span><span class=o>.</span><span class=n>eps</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>b_2</span>
</span></span></code></pre></td></tr></table></div></div><p>LayerNorm：假设数据为[batch_size, unit, 1, features]，这里是对整个样本进行 normalization。这里的 Layer Normalization 不是 Batch Normalization。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>x -&gt; attention(x) -&gt; x+self-attention(x)[残差] -&gt; layernorm(x+self-attention(x)) =&gt; y
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>y -&gt; dense(y) -&gt; y+dense(y) -&gt; layernorm(y+dense(y)) =&gt; z(输入下一层)
</span></span></code></pre></td></tr></table></div></div><p>这里稍微做了一点修改， 在 self-attention 和 dense 之后加了一个 dropout 层。另一个不同支持就是把 layernorm 层放到前面了。这里的模型为：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>x -&gt; layernorm(x) -&gt; attention(layernorm(x)) -&gt; a + attention(layernorm(x)) =&gt; y
</span></span><span class=line><span class=cl>y -&gt; layernorm(y) -&gt; dense(layernorm(y)) -&gt; y+dense(layernorm(y))
</span></span></code></pre></td></tr></table></div></div><p>原始论文的 layernorm 放在最后；而这里把它放在最前面并且在 Encoder 的最后在加了一个 layernorm。这里的实现和论文的实现基本是一致的，只是给最底层的输入 x 多做了一个 LayerNorm，而原始论文是没有的。下面是 Encoder 中的 forward 方法，这样比读者可能会比较清楚为什么 N 个 EncoderLayer 处理完成后还需要一个 LayerNorm。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>不管是 Self-Attention 还是全连接层，都首先是 LayerNorm，然后是 Self-Attention/Dense，然后是 Dropout，最好是残差连接。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>SublayerConnection</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>	LayerNorm+sublayer(Self-Attention/Dense) + dropout + 残差连接
</span></span></span><span class=line><span class=cl><span class=s2>	为了简单，把LayerNorm放到了前面，这和原始论文稍有不同，原始论文LayerNorm在最后。
</span></span></span><span class=line><span class=cl><span class=s2>	&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>supper</span><span class=p>(</span><span class=n>SublayerConnection</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Droupout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>sublayer</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=c1># sublayer是传入的参数,之后进行残差连接</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=n>x</span><span class=o>+</span><span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>sublayer</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)))</span>
</span></span></code></pre></td></tr></table></div></div><p>Self-Attention 或者 Dense 并不在这里进行构造，而是放在了 EncoderLayer 里，在 forward 的时候由 EncoderLayer 传入。这样的好处是更加通用，比如 Decoder 也是类似的需要在 Self-Attention、Attention 或者 Dense 前面加上 LayerNorm 和 Dropout 以及残差连接，我们就可以复用代码。但是这里要求传入的 sublayer 可以使用一个参数来调用的函数。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>EncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=c1># EncoderLayer由self-attn和feed_forward组成</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span> <span class=n>self_attn</span><span class=p>,</span> <span class=n>feed_forward</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>(</span><span class=n>EncoderLayer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>size</span> <span class=o>=</span> <span class=n>size</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>self_attn</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>feed_forward</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span> <span class=o>=</span> <span class=n>clones</span><span class=p>(</span><span class=n>SublayerConnection</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>dropout</span><span class=p>),</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span><span class=p>[</span><span class=mi>0</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span><span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>x</span><span class=p>,</span><span class=n>x</span><span class=p>,</span><span class=n>mask</span><span class=p>))</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span><span class=p>[</span><span class=mi>1</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>为了复用，这里的 self_attn 层和 feed_forward 层也是传入的参数，这里只构造两个 SublayerConnection。forward 调用 sublayer<a href=%e8%bf%99%e6%98%afSubLayerConnection%e5%af%b9%e8%b1%a1 rel>0</a>的<strong>call</strong>方法，最终会调到它的 forward 方法，而这个方法需要两个参数，一个是输入 Tensor， 一个是一个 callable, 并且这个 callable 可以用一个参数来调用。而 self_attn 函数需要 4 个参数（Query 的输入，key 的输入，Value 的输入和 Mask），因此这里我们使用 lambda 的技巧把它变成一个参数 x 的函数(mask 可以堪称已知的数)。因为 lambda 的形参也叫 x.</p><blockquote><p>Decoder</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Decoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>layer</span><span class=p>,</span> <span class=n>N</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>(</span><span class=n>Decoder</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>layers</span> <span class=o>=</span> <span class=n>clones</span><span class=p>(</span><span class=n>layer</span><span class=p>,</span> <span class=n>N</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>norm</span> <span class=o>=</span> <span class=n>LayerNorm</span><span class=p>(</span><span class=n>layer</span><span class=o>.</span><span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=n>layer</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>layers</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=n>x</span> <span class=o>=</span> <span class=n>layer</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Decoder 也是 N 个 DecoderLayer 的 stack，参数 layer 是 DecoderLayer，它也是一个 callable，最终<strong>call</strong>会调用 DecoderLayer.forward 方法，这个方法需要 4 个参数，输入 x, Encoder 层的输出 memory, 输入 Encoder 的 Mask(src_mask)和输入 Decoder 的 Mask(tgt_mask)。所有这里的 Decoder 的 forward 也需要 4 个参数。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>DecoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=c1># Decoder包括self-attn, src-attn, feed_forward</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span> <span class=n>self_attn</span><span class=p>,</span> <span class=n>src_attn</span><span class=p>,</span> <span class=n>feed_forward</span><span class=p>,</span> <span class=n>dropout</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>(</span><span class=n>DecoderLayer</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>size</span> <span class=o>=</span> <span class=n>size</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>self_attn</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>src_attn</span> <span class=o>=</span> <span class=n>src_attn</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>feed_forward</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span> <span class=o>=</span> <span class=n>clones</span><span class=p>(</span><span class=n>SublayerConnection</span><span class=p>(</span><span class=n>size</span><span class=p>,</span> <span class=n>dropout</span><span class=p>),</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>memory</span><span class=p>,</span> <span class=n>src_mask</span><span class=p>,</span> <span class=n>tgt_mask</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>m</span> <span class=o>=</span> <span class=n>memory</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span><span class=p>[</span><span class=mi>0</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>x</span><span class=p>,</span><span class=n>x</span><span class=p>,</span><span class=n>tgt_mask</span><span class=p>))</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span><span class=p>[</span><span class=mi>1</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=bp>self</span><span class=o>.</span><span class=n>src_attn</span><span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>m</span><span class=p>,</span><span class=n>m</span><span class=p>,</span><span class=n>src_mask</span><span class=p>))</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>sublayer</span><span class=p>[</span><span class=mi>2</span><span class=p>](</span><span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>DecoderLayer 比 EncoderLayer 多了一个 src-attn 层，这是 Decoder 时 attend to Encoder 的输出(memory)。src_attn 和 self_attn 的实现是一样的，只不过使用的 Query, Key 和 Value 的输入不同。普通的 Attention(src_attn)的 Query 是从下层输入进行来的。 Key 和 Value 是 Encoder 最后一层的输出 memory;而 Self-Attention 的 Query, Key 和 Value 都是来自下层输入进来的。</p><p>Decoder 和 Encoder 有一个关键的不同：Decoder 在解码第 t 个时刻的时候只能用$1 \cdots t$时刻的输入，而不能使用$t+1$时刻及其之后的输入。因此我们需要一个函数来产生一个 Mask 矩阵,代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>subsequent_mask</span><span class=p>(</span><span class=n>size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=c1># mask out subsequent positoins</span>
</span></span><span class=line><span class=cl>	<span class=n>attn_shape</span> <span class=o>=</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>size</span><span class=p>,</span> <span class=n>size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>subsequent_mask</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>triu</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>attn_shape</span><span class=p>),</span><span class=n>k</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=s1>&#39;uint8&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>subsequent_mask</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span>
</span></span></code></pre></td></tr></table></div></div><p>我们阅读代码之前先看它的输出：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>subsequent_mask</span><span class=p>(</span><span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># 输出</span>
</span></span><span class=line><span class=cl><span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span> <span class=mi>1</span>
</span></span></code></pre></td></tr></table></div></div><p>我们发现它输出的是一个方阵，对角线和下面都是 1。第一行只有第一列是 1，它的意思是时刻 1 只能 attend to 输入 1， 第三行说明时刻 3 可以 attend to ${1, 2, 3 }$而不能 attend to ${4,5}$的输入，因为在真正 Decoder 的时候这是属于 Future 的信息。</p><h2 id=multiheadedattention-多头注意力机制>MultiHeadedAttention 多头注意力机制</h2><p>Attention(包括 Self-Attention 和普通的 Attention)可以堪称一个函数，它的输入是 Query，Key，Value 和 Mask，输出是一个 Tensor。其中输出是 Value 的加权平均，而权重来自 Query 和 Key 的计算。具体的计算如下图所示，计算公式为：$$Attention(Q,K,V) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$$</p><p><img class=lazyload src=/svg/loading.min.svg data-src=http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_33_0.png data-srcset="http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_33_0.png, http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_33_0.png 1.5x, http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_33_0.png 2x" data-sizes=auto alt=http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_33_0.png title=Attention计算图></p><p>代码为：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>attention</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span><span class=n>dropout</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>d_k</span> <span class=o>=</span> <span class=n>query</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>query</span><span class=p>,</span><span class=n>key</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span><span class=o>/</span><span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=n>scores</span> <span class=o>=</span> <span class=n>scores</span><span class=o>.</span><span class=n>mask_fill</span><span class=p>(</span><span class=n>mask</span><span class=o>==</span><span class=mi>0</span><span class=p>,</span><span class=o>-</span><span class=mf>1e9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>p_attn</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>if</span> <span class=n>dropout</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=n>p_attn</span> <span class=o>=</span> <span class=n>dropout</span><span class=p>(</span><span class=n>p_attn</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>p_attn</span><span class=p>,</span><span class=n>value</span><span class=p>),</span><span class=n>p_attn</span>
</span></span></code></pre></td></tr></table></div></div><p>这里主要的疑问是在<code>score.mask_fill</code>，主要用于把 mask 是 0 的变成一个很小的数，这样后面经过 softmax 之后的概率就很接近零（但是理论上还是用了很少一点点未来的信息）。</p><p>之前介绍过，对于每一个 Head，都是用三个矩阵$W^{Q}$，$W^{K}$，$W^{V}$把输入转换成 Q，K 和 V。然后分别用每一个 Head 进行 Self- Attention 的计算，最后把 N 个 Head 的输出拼接起来，最后用一个矩阵$W^{O}$把输出压缩一下。具体计算框架为：</p><p><img class=lazyload src=/svg/loading.min.svg data-src=http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_38_0.png data-srcset="http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_38_0.png, http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_38_0.png 1.5x, http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_38_0.png 2x" data-sizes=auto alt=http://fancyerii.github.io/img/transformer_codes/the-annotated-transformer_38_0.png title="Multi-Head Self-Attention"></p><p>代码如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>dropout</span><span class=o>=</span><span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=nb>super</span><span class=p>(</span><span class=n>MultiHeadAttention</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=k>assert</span> <span class=n>d_model</span> <span class=o>%</span> <span class=n>h</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>h</span> <span class=c1># 这里是整除</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>h</span> <span class=o>=</span> <span class=n>h</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>linears</span> <span class=o>=</span> <span class=n>clones</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span><span class=n>d_k</span><span class=p>),</span><span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>attn</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>p</span><span class=o>=</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>foward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=c1># 所有h个head的mask都是相同的</span>
</span></span><span class=line><span class=cl>			<span class=n>mask</span> <span class=o>=</span> <span class=n>mask</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=n>nbatches</span> <span class=o>=</span> <span class=n>query</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=c1># 1) 首先使用线性变换，然后把d_model分配给h个Head，每个head为d_k=d_model/h</span>
</span></span><span class=line><span class=cl>		<span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span> <span class=o>=</span> <span class=p>[</span><span class=n>l</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>nbatches</span><span class=p>,</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=bp>self</span><span class=o>.</span><span class=n>h</span><span class=p>,</span><span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>)</span> <span class=k>for</span> <span class=n>l</span><span class=p>,</span><span class=n>x</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>linears</span><span class=p>,(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>))]</span>
</span></span><span class=line><span class=cl>		<span class=c1># 2) 使用attention函数计算</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>attn</span> <span class=o>=</span> <span class=n>attention</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>,</span><span class=n>dropout</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=c1># 3）</span>
</span></span><span class=line><span class=cl>		<span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>nbatches</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=bp>self</span><span class=o>.</span><span class=n>h</span><span class=o>*</span><span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>linears</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>](</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>我们首先来看构造函数， 这里 d_model(512)是 Multi-Head 的输出大小，因为有 h(8)个 head， 因此每个 head 的 d_k=512/8=64。接着我们构造 4 个(d_model $*$ d_model)的矩阵，后面我们会看到它的用处。最后是构造一个 Dropout 层。</p><p>然后我们来看 forward 方法。输入的 mask 是（batch,1,time）的，因为每个 head 的 mask 都是一样的，所以先用 unsqueeze(1)变成(batch,1,1,time)，mask 我们前面已经分析过了。</p><p>接下来就是根据输入的 query, key, value 计算变换后的 Multi-Head 的 query, key 和 value。<code>zip(self.linears, (query,key,value))</code>是把<code>(self.linear[0], self.linear[1], self.linear[2])</code>和<code>(query, key, value)</code>放在一起遍历。我们可以只看一个<code>self.linear[0] (query)</code>。根据构造函数的定义，<code>self.linear[0]</code>是一个<code>[512,512]</code>的矩阵，而<code>query</code>是<code>(batch, time, 512)</code>，相乘之后得到的新 query 还是 512 维的向量，然后用 view 把它变成<code>(batch, time, 8, 64)</code>。然后 transpose 成(batch, 8, time, 64)，这是 attention 函数要求的 shape。分别对 8 个 Head，每个 Head 的 Query 都是 64 维。最后使用<code>self.linear[-1]</code>对<code>x</code>进行线性变换，<code>self.linear[-1]</code>是<code>[512, 512]</code>的，因此最终的输出还是<code>(batch, time, 512)</code>。</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 0001-01-01</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://ethan-phu.github.io/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/ data-title=transformer源码阅读 data-hashtags=技术,深度学习,笔记,源码阅读><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://ethan-phu.github.io/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/ data-hashtag=技术><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 Hacker News" data-sharer=hackernews data-url=https://ethan-phu.github.io/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/ data-title=transformer源码阅读><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 Line" data-sharer=line data-url=https://ethan-phu.github.io/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/ data-title=transformer源码阅读><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@6.20.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://ethan-phu.github.io/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/ data-title=transformer源码阅读><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/%E6%8A%80%E6%9C%AF/>技术</a>,&nbsp;<a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a>,&nbsp;<a href=/tags/%E7%AC%94%E8%AE%B0/>笔记</a>,&nbsp;<a href=/tags/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/>源码阅读</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%8F%91%E5%B1%95%E7%9A%84%E6%80%9D%E8%80%83/ class=prev rel=prev title=对个人发展的思考><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>对个人发展的思考</a>
<a href=/%E5%85%B0%E8%BF%AA%E6%95%99%E6%8E%88%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E8%AF%BE/ class=next rel=next title="Randy Pausch 教授的最后一课">Randy Pausch 教授的最后一课<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/ethan-phu target=_blank>ethan</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/lightgallery@2.4.0/css/lightgallery-bundle.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=/lib/lunr/lunr.stemmer.support.min.js></script><script type=text/javascript src=/lib/lunr/lunr.zh.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.1/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.4.0/lightgallery.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.4.0/plugins/thumbnail/lg-thumbnail.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.4.0/plugins/zoom/lg-zoom.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:100},comment:{},lightgallery:!0,search:{highlightTag:"em",lunrIndexURL:"/index.json",lunrLanguageCode:"zh",lunrSegmentitURL:"/lib/lunr/lunr.segmentit.js",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-TYXZ2Y2RHZ")</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-TYXZ2Y2RHZ" async></script></body></html>