[{"categories":["成长"],"content":"这是一个测试问题 ","date":"2022-05-30","objectID":"/%E6%97%B6%E9%97%B4/:0:0","tags":["测试"],"title":"时间戳测试","uri":"/%E6%97%B6%E9%97%B4/"},{"categories":["成长"],"content":"在重听李沐老师的《Resnet 论文精读》这一课的时候，ps:「之前没有好好读，:\u003e)羞耻」。在讲到双栏论文中第一页的第二栏最上面，这个位置在学术界是非常重要的。提到了 Randy Pausch 教授在图形学开创了这一风格的写法，然后提到了他的最后一课「深刻印象」。 于是我从 cmu 网站中找到了当年演讲的材料，并完整的看了视频，这份笔记是 Randy 的人生经验和建议的抄录。 ","date":"2022-05-29","objectID":"/%E5%85%B0%E8%BF%AA%E6%95%99%E6%8E%88%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E8%AF%BE/:0:0","tags":["成长","思考","人生建议"],"title":"Randy Pausch 教授的最后一课","uri":"/%E5%85%B0%E8%BF%AA%E6%95%99%E6%8E%88%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E8%AF%BE/"},{"categories":["成长"],"content":"人生经验 Have something to bring to the table, because that will make you more welcom. 你必须要有一些真本领，这样可以让你更受欢迎。 You’ve got to get the fundamentals down because otherwise the fancy stuff isn’t going to work. 你必须练好基本功，否则后面的事情都不会发生。 That was a bit of a setback. But remember, the brick walls are there for a reason. The brick walls are not there to keep us out. The brick walls are there to give us a chance to show how badly we want something. Becuase the brick walls are there to stop the people who don’t want it badly enough. They’re there to stop the other people. Remember brick walls let us show our dedication. They are there to separate us from the people who don’t really want to achieve their childhood dreams. 你总会遇到挫折。但是记住，它们的出现不是没有原因的。砖墙并不是为了挡住我们。它在那里，只是为了测试，我们的决心到底有多迫切。它在那里挡住那些没有强烈决心的人。它不让那些人通过。记住，砖墙的存在是为了显示我们自己付出的决心。它使得我们，同那些并不真的想实现梦想的人得以区分。 ","date":"2022-05-29","objectID":"/%E5%85%B0%E8%BF%AA%E6%95%99%E6%8E%88%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E8%AF%BE/:1:0","tags":["成长","思考","人生建议"],"title":"Randy Pausch 教授的最后一课","uri":"/%E5%85%B0%E8%BF%AA%E6%95%99%E6%8E%88%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E8%AF%BE/"},{"categories":["成长"],"content":"为人处世的建议 helping others. 帮助他人。 never lose the childlike wonder. It’s what drives us. 永远不要失去好奇心，它是人类前进的动力。 Loyalty is a two way street. 诚以待人，这样别人也会忠实的对待你。 Never give up. 永远不要放弃 You can’t get there alone. People have to help you. You get people to help you by telling the truth. 你不能单打独斗，必须有人来帮你。只要你讲真话，就会有人来帮你。 Apologize when you screw up and focus on other people, not on yourself. 当你把事情搞砸，首先要向别人道歉，首先关心他们的损失，而不是你自己的损失。 When you do the right thing, good stuff has a way of happening. 如果你做了正确的事，好的结果自然会发生。 Get a feedback loop and listen to it. 注意倾听反馈。 Show gratitude. 感恩 Don’t complain. Just work harder. 不要抱怨，而要加倍努力。 Be good at something, it makes you valuable. 要有一技之长，它使你有价值。 Work hard. 努力再努力。 Find the best in everybody. 注意发现他人的优点。 Be prepared. Luck is truly where preparation meets opportunity. 做好准备。所谓幸运，真的是机会和准备的结合。 ","date":"2022-05-29","objectID":"/%E5%85%B0%E8%BF%AA%E6%95%99%E6%8E%88%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E8%AF%BE/:2:0","tags":["成长","思考","人生建议"],"title":"Randy Pausch 教授的最后一课","uri":"/%E5%85%B0%E8%BF%AA%E6%95%99%E6%8E%88%E7%9A%84%E6%9C%80%E5%90%8E%E4%B8%80%E8%AF%BE/"},{"categories":["技术"],"content":"本文介绍 Tranformer 的代码。 ","date":"2022-05-28","objectID":"/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/:0:0","tags":["技术","深度学习","笔记","源码阅读"],"title":"transformer源码阅读","uri":"/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"},{"categories":["技术"],"content":"模型结构 Encoder 将输入序列$(x_{1},\\cdots,x_{n})$ 映射成一个连续的序列$z = (z_{1},\\cdots,z_{n})$。而 Decoder 根据$z$来解码得到输出序列$(y_{1},\\cdots,y_{m})$。Decoder 是自回归的(auto-regressive)–它会把前一个时刻的输出作为当前时刻的输入。Encoder-Decoder 结构模型的代码如下： class EncoderDecoder(nn.Module): \"\"\" 标准的Encoder-Decoder架构。 \"\"\" def __init__(self, encoder, decoder, src_embed, tgt_embed, generator): super(EncoderDecoder,self).__init__() self.encoder = encoder self.decoder = decoder # 源语言和目标语言的embedding self.src_embed = src_embed self.tgt_embed = tgt_embed # generator主要是根据Decoder的隐状态输出当前时刻的词(单个词) # 基本的实现就是隐状态输入一个全连接层，然后接一个softmax变成概率 self.generator = generator def forward(self, src, tgt, src_mask, tgt_mask): # 首先调用encode方法对输入进行编码，然后调用decode方法进行解码 return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) def encode(self, src, src_mask): # 调用self.encoder函数 return self.encoder(self.src_embed(src), src_mask) def decode(self, memory, src_mask, tgt, tgt_mask): # 调用self.decoder函数 注意⚠️：这里定义的memery是encoder的输出结果。 return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) EncoderDecoder 定义了一种通用的 Encoder-Decoder 架构，具体的 Encoder、Decoder、src_embed、target_embed 和 generator 都是构造函数传入的参数。这样我们做实验更换不同的组件就会更加方便。 class Generator(nn.Module): def __init__(self, d_model, vocab): super(Generator, self).__init__() # d_model是Decoder输出的大小，vocab是词典的大小 self.proj = nn.Linear(d_model, vocab) def forward(self, x): return F.log_softmax(self.proj(x), dim=-1) 注意 ⚠️：Generator 返回的是 softmax 的 log 值。在 pytorch 中为了计算交叉熵损失，有两种方法。第一种方法就是 nn.CrossEntropyLoss(),一种是使用 NLLLoss()。第一种方法更加容易懂，但是在很多开源代码里第二种更常见。 CrossEntropyLoss: criterion = nn.CrossEntropyLoss() x = torch.randn(1,5) # 服从0-1的正太分布。 y = torch.empty(1, dtype = torch.long).random_(5) loss = criterion(x,y) 比如上面的代码，假设是 5 分类问题，x表示模型的输出 logits(batch=1)，而 y 是真实分类的下标(0-4)。实际的计算过程为：$loss = -\\sum^{5}{i=1}y{i}log(softmax(x_{i}))$。 NLLLoss(Negative Log Likelihood Loss)是计算负 log 似然损失。它输入的 x 是 log_softmax 之后的结果（长度为 5 的数组），y 是真实分类（0-4），输出就是 x[y]。因此代码为： m = F.log_softmax(x, dim=1) criterion = nn.NLLLoss() x = torch.randn(1, 5) y = torch.empty(1, dtype = torch.long).random_(5) loss = criterion(m(x), y) Transformer 模型也是遵循上面的架构，只不过它的 Encoder 是 N(6)个 EncoderLayer 组成，每个 EncoderLayer 包含一个 Self-Attention SubLayer 层和一个全连接 SubLayer 层。而它的 Decoder 也是 N(6)个 DecoderLayer 组成，每个 DecoderLayer 包含一个 Self-Attention SubLayer 层、Attention SubLayer 层和全连接 SubLayer 层。如下图所示。 ","date":"2022-05-28","objectID":"/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/:1:0","tags":["技术","深度学习","笔记","源码阅读"],"title":"transformer源码阅读","uri":"/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"},{"categories":["技术"],"content":"Encoder 和 Decoder Stack 前面说了 Encoder 和 Decoder 都是由 N 个相同结构的 Layer 堆积(stack)而成。因此我们首先定义 clones 函数，用于克隆相同的 SubLayer。 def clones(module, N): # 克隆N个完全相同的SubLayer，使用了copy.deepcopy return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) 这里使用了 nn.ModuleList, ModuleList 就像一个普通的 Python 的 List，我们可以使用下标来访问它，它的好处是传入的 ModuleList 的所有 Module 都会注册的 PyTorch 里，这样 Optimizer 就能找到这里面的参数，从而能够用梯度下降更新这些参数。但是 nn.ModuleList 并不是 Module（的子类），因此它没有 forward 等方法，我们通常把它放到某个 Module 里。接下来定义 Encoder: class Encoder(nn.Module): # Encoder是N个EncoderLayer的stack def __init__(self, layer, N): super(Encoder, self).__init__() # layer是一个SubLayer，我们clone N个 self.layers = clones(layer, N) # 再加一个LayerNorm层 self.norm = LayerNorm(layer.size) def forward(self, x, mask): # 逐层进行处理 for layer in self.layers: x = layer(x, mask) # 最后进行LayerNorm return self.norm(x) Encoder 就是 N 个 SubLayer 的 stack，最后加上一个 LayerNorm。我们来看 LayerNorm: class LayerNorm(nn.Module): def __init__(self, features, eps=1e-6): super(LayerNorm, self).__init__() self.a_2 = nn.Parameter(torch.ones(features)) self.b_2. = nn.Parameter(torch.zeros(feagures)) self.eps = eps def forward(self, x): mean = x.mean(-1, keepdim = True) std = x.std(-1, keepdim = True) return self.a_2 * (x - mean)/(std+self.eps) + self.b_2 LayerNorm：假设数据为[batch_size, unit, 1, features]，这里是对整个样本进行 normalization。这里的 Layer Normalization 不是 Batch Normalization。 x -\u003e attention(x) -\u003e x+self-attention(x)[残差] -\u003e layernorm(x+self-attention(x)) =\u003e y y -\u003e dense(y) -\u003e y+dense(y) -\u003e layernorm(y+dense(y)) =\u003e z(输入下一层) 这里稍微做了一点修改， 在 self-attention 和 dense 之后加了一个 dropout 层。另一个不同支持就是把 layernorm 层放到前面了。这里的模型为： x -\u003e layernorm(x) -\u003e attention(layernorm(x)) -\u003e a + attention(layernorm(x)) =\u003e y y -\u003e layernorm(y) -\u003e dense(layernorm(y)) -\u003e y+dense(layernorm(y)) 原始论文的 layernorm 放在最后；而这里把它放在最前面并且在 Encoder 的最后在加了一个 layernorm。这里的实现和论文的实现基本是一致的，只是给最底层的输入 x 多做了一个 LayerNorm，而原始论文是没有的。下面是 Encoder 中的 forward 方法，这样比读者可能会比较清楚为什么 N 个 EncoderLayer 处理完成后还需要一个 LayerNorm。 def forward(self, x, mask): for layer in self.layers: x = layer(x, mask) return self.norm(x) 不管是 Self-Attention 还是全连接层，都首先是 LayerNorm，然后是 Self-Attention/Dense，然后是 Dropout，最好是残差连接。 class SublayerConnection(nn.Module): \"\"\" LayerNorm+sublayer(Self-Attention/Dense) + dropout + 残差连接 为了简单，把LayerNorm放到了前面，这和原始论文稍有不同，原始论文LayerNorm在最后。 \"\"\" def __init__(self, size, dropout): supper(SublayerConnection, self).__init__() self.norm = LayerNorm(size) self.dropout = nn.Droupout(dropout) def forward(self, x, sublayer): # sublayer是传入的参数,之后进行残差连接 return x+self.dropout(sublayer(self.norm(x))) Self-Attention 或者 Dense 并不在这里进行构造，而是放在了 EncoderLayer 里，在 forward 的时候由 EncoderLayer 传入。这样的好处是更加通用，比如 Decoder 也是类似的需要在 Self-Attention、Attention 或者 Dense 前面加上 LayerNorm 和 Dropout 以及残差连接，我们就可以复用代码。但是这里要求传入的 sublayer 可以使用一个参数来调用的函数。 class EncoderLayer(nn.Module): # EncoderLayer由self-attn和feed_forward组成 def __init__(self, size, self_attn, feed_forward, dropout): super(EncoderLayer, self).__init__() self.size = size self.self_attn = self_attn self.feed_forward = feed_forward self.sublayer = clones(SublayerConnection(size, dropout), 2) def forward(self, x, mask): x = self.sublayer[0](x, lambda x:self.self_attn(x,x,x,mask)) return self.sublayer[1](x, self.feed_forward) 为了复用，这里的 self_attn 层和 feed_forward 层也是传入的参数，这里只构造两个 SublayerConnection。forward 调用 sublayer0的call方法，最终会调到它的 forward 方法，而这个方法需要两个参数，一个是输入 Tensor， 一个是一个 callable, 并且这个 callable 可以用一个参数来调用。而 self_attn 函数需要 4 个参数（Query 的输入，key 的输入，Value 的输入和 Mask），因此这里我们使用 lambda 的技巧把它变成一个参数 x 的函数(mask 可以堪称已知的数)。因为 lambda 的形参也叫 x. Decoder class Decoder(nn.Module): def __init__(self, layer, N): super(Decoder, self).__init__() self.layers = clones(layer, N) self.norm = LayerNorm(layer.size) def forward(self, x, memory, src_mask, tgt_mask): for layer in self.layers: x = layer(x, memory, src_mask, tgt_mask) return self.norm(x) Decoder 也是 N 个 DecoderLayer 的 stack，参数 layer 是 DecoderLayer，它也是一个 callable，最终call会调用 DecoderLayer.forward 方法，这个方法需要 4 个参数，输入 x, Encoder 层的输出 memory, 输入 Encoder 的 Mask(src_mask)和输入 Decoder 的 ","date":"2022-05-28","objectID":"/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/:2:0","tags":["技术","深度学习","笔记","源码阅读"],"title":"transformer源码阅读","uri":"/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"},{"categories":["技术"],"content":"MultiHeadedAttention 多头注意力机制 Attention(包括 Self-Attention 和普通的 Attention)可以堪称一个函数，它的输入是 Query，Key，Value 和 Mask，输出是一个 Tensor。其中输出是 Value 的加权平均，而权重来自 Query 和 Key 的计算。具体的计算如下图所示，计算公式为：$$Attention(Q,K,V) = softmax(\\frac{QK^{T}}{\\sqrt{d_{k}}})V$$ 代码为： def attention(query, key, value, mask=None,dropout=None): d_k = query.size(-1) scores = torch.matmul(query,key.transpose(-2,-1))/math.sqrt(d_k) if mask is not None: scores = scores.mask_fill(mask==0,-1e9) p_attn = F.softmax(scores, dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn,value),p_attn 这里主要的疑问是在score.mask_fill，主要用于把 mask 是 0 的变成一个很小的数，这样后面经过 softmax 之后的概率就很接近零（但是理论上还是用了很少一点点未来的信息）。 之前介绍过，对于每一个 Head，都是用三个矩阵$W^{Q}$，$W^{K}$，$W^{V}$把输入转换成 Q，K 和 V。然后分别用每一个 Head 进行 Self- Attention 的计算，最后把 N 个 Head 的输出拼接起来，最后用一个矩阵$W^{O}$把输出压缩一下。具体计算框架为： 代码如下： class MultiHeadAttention(nn.Module): def __init__(self, h, d_model, dropout=0.1): super(MultiHeadAttention, self).__init__() assert d_model % h == 0 self.d_k = d_model // h # 这里是整除 self.h = h self.linears = clones(nn.Linear(d_model,d_k),4) self.attn = None self.dropout = nn.Dropout(p=dropout) def foward(self, query, key, value, mask=None): if mask is not None: # 所有h个head的mask都是相同的 mask = mask.unsqueeze(1) nbatches = query.size(0) # 1) 首先使用线性变换，然后把d_model分配给h个Head，每个head为d_k=d_model/h query, key, value = [l(x).view(nbatches,-1,self.h,self.d_k).transpose(1,2) for l,x in zip(self.linears,(query, key, value))] # 2) 使用attention函数计算 x, self.attn = attention(query, key, value, mask=mask,dropout = self.dropout) # 3） x = x.transpose(1,2).contiguous().view(nbatches, -1,self.h*self.d_k) return self.linears[-1](x) 我们首先来看构造函数， 这里 d_model(512)是 Multi-Head 的输出大小，因为有 h(8)个 head， 因此每个 head 的 d_k=512/8=64。接着我们构造 4 个(d_model $*$ d_model)的矩阵，后面我们会看到它的用处。最后是构造一个 Dropout 层。 然后我们来看 forward 方法。输入的 mask 是（batch,1,time）的，因为每个 head 的 mask 都是一样的，所以先用 unsqueeze(1)变成(batch,1,1,time)，mask 我们前面已经分析过了。 接下来就是根据输入的 query, key, value 计算变换后的 Multi-Head 的 query, key 和 value。zip(self.linears, (query,key,value))是把(self.linear[0], self.linear[1], self.linear[2])和(query, key, value)放在一起遍历。我们可以只看一个self.linear[0] (query)。根据构造函数的定义，self.linear[0]是一个[512,512]的矩阵，而query是(batch, time, 512)，相乘之后得到的新 query 还是 512 维的向量，然后用 view 把它变成(batch, time, 8, 64)。然后 transpose 成(batch, 8, time, 64)，这是 attention 函数要求的 shape。分别对 8 个 Head，每个 Head 的 Query 都是 64 维。最后使用self.linear[-1]对x进行线性变换，self.linear[-1]是[512, 512]的，因此最终的输出还是(batch, time, 512)。 ","date":"2022-05-28","objectID":"/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/:3:0","tags":["技术","深度学习","笔记","源码阅读"],"title":"transformer源码阅读","uri":"/transformer%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"},{"categories":["思考"],"content":"这一段时间我也一直在思考，毕业之后的发展。读了许多大佬写的博客，想着能够从中汲取一些经验和启发。发现大家都有一个共性就是善于总结和对自己的职业生涯进行了一定的规划，他们清楚的了解自己的定位和未来想要得到什么。我一直坚信花时间来整理状态，是一件很值得投资的事情。这篇文章也相当于研究生生涯总结和职业生涯的开篇吧！ 毕业前的时光算得上是踏上职场前最后的悠闲。尽管对未来充满了希望，与此同时，我的内心也充斥着彷徨。害怕在未来几年一无所成，害怕自己达不到自己所期待的那种境界。每年设定目标，然而能够完成的却寥寥无几。才有了现在对未来的迷茫，心中一腔热血却像苍蝇一样，漫无目的。这可能也是普通人和大佬之间的区别吧！阅读过大佬的文章后，有下面的启示。 ","date":"2022-05-15","objectID":"/%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%8F%91%E5%B1%95%E7%9A%84%E6%80%9D%E8%80%83/:0:0","tags":["成长","反思","思考"],"title":"对个人发展的思考","uri":"/%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%8F%91%E5%B1%95%E7%9A%84%E6%80%9D%E8%80%83/"},{"categories":["思考"],"content":"别多想，多实践 你思考的 90%的问题，至少 1000 年前，先哲们都给了答案。 剩下的 10%，300 年前的哲学家，思想家一定给出了答案。 绝对不存在你思考的问题，历史上的思想家没有给出答案。所以大多数的时候，没看书之前，你的思考，是徒劳的。 大多数时候，我们所思考的问题，前人都已经思考过了。我们需要做的是，踏踏实实的沉下心来，进行学习即可。我们总在思考未来应该如何做才能快速的成长，追求完成一件事情的形式。殊不知未来正是由无数个当下的事情组成的。与其担心未来，还不如沉下心来把当下的事情做到极致，未来一定会出现在你的眼前。 ","date":"2022-05-15","objectID":"/%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%8F%91%E5%B1%95%E7%9A%84%E6%80%9D%E8%80%83/:1:0","tags":["成长","反思","思考"],"title":"对个人发展的思考","uri":"/%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%8F%91%E5%B1%95%E7%9A%84%E6%80%9D%E8%80%83/"},{"categories":["思考"],"content":"尽早规划（长线思维） 怎么过一天，就怎么过一生，如果认为明天或一年之后会有所改变，那么今天的自己是一年前希望看到的自己么。 随着时间的推移，资产（你认为有价值的一切）变得更有价值还是廉价 把每一个场景都看成投资场景，每一个行为当作投资行为，重视它对现在及将来的影响 咱们中国有一个不变的传统就是“五年规划”，国家尚且如此，何况人？所以我们也要及早的对自己进行一个五年规划，这个规划应该是方向性的，不是细节的。细节应当是每年每季度每月具体去完成以达到五年规划中的目标。 ","date":"2022-05-15","objectID":"/%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%8F%91%E5%B1%95%E7%9A%84%E6%80%9D%E8%80%83/:2:0","tags":["成长","反思","思考"],"title":"对个人发展的思考","uri":"/%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%8F%91%E5%B1%95%E7%9A%84%E6%80%9D%E8%80%83/"},{"categories":["思考"],"content":"注重输出 只是看书或者视频，容易造成已经理解了某个知识点的错觉，短期记忆未经加固，很快就会「挥发」 无输出不输入，输出的方式可以是文章或者视频或者闲聊，经过强化后的内容更容易进入长期记忆 输出的过程会联结之前的积累，让知识更扎实，输出过程也会更流畅，输入和输出的比例可以控制在 3:7 在知识库系统的过程中，我们往往过于完美主义，想把知识系统工具一次性构建完整，然后把大量的时间都放在了工具的搭建中，折腾了很多中工具：notion, obsidian, logseq 等。我个人认为，工具是在自己不断的使用的过程中完善的，知识库也是一样，首先我们要有输出，之后在慢慢的进行调整，最后会实现一个理想中适合自己的知识系统的。我在这里建议放弃完美主义，在实践中不断的调整自己调整工具。还有一个就是逃离算法推荐吧，拥抱内容的多元化。 不要只停留在理论上，去实践，会发现更多的问题和挑战，也更有趣味，“what i cannot create, i do not understand”。 ","date":"2022-05-15","objectID":"/%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%8F%91%E5%B1%95%E7%9A%84%E6%80%9D%E8%80%83/:3:0","tags":["成长","反思","思考"],"title":"对个人发展的思考","uri":"/%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%8F%91%E5%B1%95%E7%9A%84%E6%80%9D%E8%80%83/"},{"categories":["思考"],"content":"参考资料 工程师的成长 写在 28 岁边上 技术同学在业务中的成长 用未来的视角来看今天 ","date":"2022-05-15","objectID":"/%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%8F%91%E5%B1%95%E7%9A%84%E6%80%9D%E8%80%83/:4:0","tags":["成长","反思","思考"],"title":"对个人发展的思考","uri":"/%E5%AF%B9%E4%B8%AA%E4%BA%BA%E5%8F%91%E5%B1%95%E7%9A%84%E6%80%9D%E8%80%83/"},{"categories":["技术"],"content":"工作中常需要连接着服务器，比如在深度学习训练模型的过程中，需要长时间连接着服务器，在一段时间没有操作后，ssh 会自动断开。 为了解决这个问题，在网上找到一种配置方法，亲测很久都不会再断开了，在此记录： 众所周知，ssh 是用于与远程服务器建立加密通信通道的，因此配置涉及到服务器和客户端： 服务端 /etc/ssh/sshd_config +ClientAliveInterval 60 # 每60秒发送一个KeepAlive请求 +ClientAliveCountMax 15 # 总时间为：15*60， 15分钟没有操作，终端断开。 # 以下命令重启 sshd 服务 service sshd reload service sshd restart 客户端 ~/.ssh/config # 修改～/.ssh/config配置，对当前用户生效 # 这样配置通配所有服务端 Host * ServerAliveInterval 60 ","date":"2022-05-13","objectID":"/ssh%E8%87%AA%E5%8A%A8%E6%96%AD%E5%BC%80%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/:0:0","tags":["技术","工具","ssh"],"title":"ssh自动断开修复方法","uri":"/ssh%E8%87%AA%E5%8A%A8%E6%96%AD%E5%BC%80%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/"},{"categories":["技术"],"content":"参考文献 SSH 长时间不使用自动断开解决方案 ","date":"2022-05-13","objectID":"/ssh%E8%87%AA%E5%8A%A8%E6%96%AD%E5%BC%80%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/:1:0","tags":["技术","工具","ssh"],"title":"ssh自动断开修复方法","uri":"/ssh%E8%87%AA%E5%8A%A8%E6%96%AD%E5%BC%80%E4%BF%AE%E5%A4%8D%E6%96%B9%E6%B3%95/"},{"categories":null,"content":"关于我 Hi, I am a third-year master student at School of HUTB and received the bachelor’s degree in measurement and control technology and instrumentation from the Harbin University of Science and Technology, Harbin, China, in 2017. I worked as a Software Development Engineer from 2017 to 2018. His main research interests include cybersecurity, artificial intelligence, and natural language processing. ","date":"2022-01-01","objectID":"/about/:0:0","tags":null,"title":"about","uri":"/about/"},{"categories":null,"content":"成果 Zhou X, Hu Y, Liang W, et al. Variational LSTM enhanced anomaly detection for industrial big data[J]. IEEE Transactions on Industrial Informatics, 2020, 17(5): 3469-3477. Zhou X, Hu Y, Wu J, et al. Distribution Bias Aware Collaborative Generative Adversarial Network for Imbalanced Deep Learning in Industrial IoT[J]. IEEE Transactions on Industrial Informatics, 2022. Liang W, Hu Y, Zhou X, et al. Variational Few-Shot Learning for Microservice-Oriented Intrusion Detection in Distributed Industrial IoT[J]. IEEE Transactions on Industrial Informatics, 2022, 18(8): 5087-5095. ","date":"2022-01-01","objectID":"/about/:1:0","tags":null,"title":"about","uri":"/about/"}]